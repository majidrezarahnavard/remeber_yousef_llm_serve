version: "3.9"

services:
  ollama-docs:
    image: llm-docs-ollama-app
    env_file:
      - .env
    networks:
      - llm-serve-network
    ports:
      - 8080:8080
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama:
      image: ollama/ollama:latest
      ports:
        - 7869:11434
      volumes:
        - .:/code
        - ./ollama/ollama:/root/.ollama
      container_name: ollama
      pull_policy: always
      tty: true
      restart: always
      environment:
        - OLLAMA_KEEP_ALIVE=24h
      networks:
        - llm-serve-network
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]


networks:
  llm-serve-network:
    external: true